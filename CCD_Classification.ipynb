{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Colonna17/Collision_Avoidance-Sapienza_Vision_and_Perception/blob/main/CCD_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-0MbHDLyFCP",
        "outputId": "94d23eff-4fd6-4375-9f18-65433fd68e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "#from Seq2Seq import Seq2Seq\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n"
      ],
      "metadata": {
        "id": "t9Lx46qyL3-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch"
      ],
      "metadata": {
        "id": "OlJW-b8ijS5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "059jdHMRDWmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV15nzv2oaKq",
        "outputId": "7a52fd6f-440e-47c3-a918-f020211449f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec 15 12:04:25 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    54W / 400W |      3MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --recurse-submodules https://github.com/Colonna17/Collision_Avoidance-Sapienza_Vision_and_Perception.git\n",
        "\n",
        "%cd Collision_Avoidance-Sapienza_Vision_and_Perception\n",
        "%pip install -qr external/yolov5/requirements.txt \n",
        "%pip install -qr requirements.txt #install dependencies"
      ],
      "metadata": {
        "id": "PqfPdaTd1VwK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41231d89-0487-40c7-9757-43032b668cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Collision_Avoidance-Sapienza_Vision_and_Perception'...\n",
            "remote: Enumerating objects: 306, done.\u001b[K\n",
            "remote: Counting objects: 100% (142/142), done.\u001b[K\n",
            "remote: Compressing objects: 100% (107/107), done.\u001b[K\n",
            "remote: Total 306 (delta 46), reused 120 (delta 26), pack-reused 164\u001b[K\n",
            "Receiving objects: 100% (306/306), 293.83 MiB | 61.47 MiB/s, done.\n",
            "Resolving deltas: 100% (92/92), done.\n",
            "Submodule 'Yolov5_StrongSORT_OSNet' (https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet.git) registered for path 'external/Yolov5_StrongSORT_OSNet'\n",
            "Submodule 'yolov5' (https://github.com/ultralytics/yolov5.git) registered for path 'external/yolov5'\n",
            "Cloning into '/content/Collision_Avoidance-Sapienza_Vision_and_Perception/external/Yolov5_StrongSORT_OSNet'...\n",
            "remote: Enumerating objects: 3356, done.        \n",
            "remote: Counting objects: 100% (246/246), done.        \n",
            "remote: Compressing objects: 100% (121/121), done.        \n",
            "remote: Total 3356 (delta 147), reused 186 (delta 122), pack-reused 3110        \n",
            "Receiving objects: 100% (3356/3356), 93.54 MiB | 45.98 MiB/s, done.\n",
            "Resolving deltas: 100% (1651/1651), done.\n",
            "Cloning into '/content/Collision_Avoidance-Sapienza_Vision_and_Perception/external/yolov5'...\n",
            "remote: Enumerating objects: 14513, done.        \n",
            "remote: Counting objects: 100% (36/36), done.        \n",
            "remote: Compressing objects: 100% (22/22), done.        \n",
            "remote: Total 14513 (delta 19), reused 24 (delta 14), pack-reused 14477        \n",
            "Receiving objects: 100% (14513/14513), 13.66 MiB | 35.24 MiB/s, done.\n",
            "Resolving deltas: 100% (9991/9991), done.\n",
            "Submodule path 'external/Yolov5_StrongSORT_OSNet': checked out '4cf9555609ae2c6c9a9dba7f4202e0a509ef90b5'\n",
            "Submodule 'strong_sort/deep/reid' (https://github.com/KaiyangZhou/deep-person-reid) registered for path 'external/Yolov5_StrongSORT_OSNet/strong_sort/deep/reid'\n",
            "Submodule 'yolov5' (https://github.com/ultralytics/yolov5.git) registered for path 'external/Yolov5_StrongSORT_OSNet/yolov5'\n",
            "Cloning into '/content/Collision_Avoidance-Sapienza_Vision_and_Perception/external/Yolov5_StrongSORT_OSNet/strong_sort/deep/reid'...\n",
            "remote: Enumerating objects: 9860, done.        \n",
            "remote: Counting objects: 100% (10/10), done.        \n",
            "remote: Compressing objects: 100% (9/9), done.        \n",
            "remote: Total 9860 (delta 1), reused 9 (delta 1), pack-reused 9850        \n",
            "Receiving objects: 100% (9860/9860), 9.57 MiB | 27.53 MiB/s, done.\n",
            "Resolving deltas: 100% (7286/7286), done.\n",
            "Cloning into '/content/Collision_Avoidance-Sapienza_Vision_and_Perception/external/Yolov5_StrongSORT_OSNet/yolov5'...\n",
            "remote: Enumerating objects: 14513, done.        \n",
            "remote: Counting objects: 100% (36/36), done.        \n",
            "remote: Compressing objects: 100% (22/22), done.        \n",
            "remote: Total 14513 (delta 19), reused 24 (delta 14), pack-reused 14477        \n",
            "Receiving objects: 100% (14513/14513), 13.66 MiB | 35.67 MiB/s, done.\n",
            "Resolving deltas: 100% (10000/10000), done.\n",
            "Submodule path 'external/Yolov5_StrongSORT_OSNet/strong_sort/deep/reid': checked out '4a0793780bd13f53ec2ca753a94dcef62dc9e955'\n",
            "Submodule path 'external/Yolov5_StrongSORT_OSNet/yolov5': checked out '91a81d48fa4e34dbdbaf0e45a1f841c11216aab5'\n",
            "Submodule path 'external/yolov5': checked out '91a81d48fa4e34dbdbaf0e45a1f841c11216aab5'\n",
            "/content/Collision_Avoidance-Sapienza_Vision_and_Perception\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 4.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 903 kB/s \n",
            "\u001b[K     |████████████████████████████████| 178 kB 8.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 138 kB 58.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 145 kB 86.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 8.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.9 MB/s \n",
            "\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Vision and Perception Project/Car Crash Dataset"
      ],
      "metadata": {
        "id": "jSrvfXBV1HXZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aba58b5d-0d60-4523-c94a-bf3083915776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1pLOgUcJNt9NBV6ziPglNJ1kPKNX3b0It/Vision and Perception Project/Car Crash Dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def read_annotations_file(anno_file):\n",
        "    assert os.path.exists(anno_file), \"Annotation file does not exist!\" + anno_file\n",
        "    result = []\n",
        "    with open(anno_file, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            items = {}\n",
        "            items['vid'] = line.strip().split(',[')[0]\n",
        "            labels = line.strip().split(',[')[1].split('],')[0]\n",
        "            items['label'] = [int(val) for val in labels.split(',')]\n",
        "            others = line.strip().split(',[')[1].split('],')[1].split(',')\n",
        "            items['startframe'], items['vid_ytb'], items['lighting'], items['weather'], items['ego_involve'] = others\n",
        "            result.append(items)\n",
        "    f.close()\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_video_frames(video_file, topN=50):\n",
        "    # get the video data\n",
        "    cap = cv2.VideoCapture(video_file)\n",
        "    ret, frame = cap.read()\n",
        "    video_data = []\n",
        "    while (ret):\n",
        "        video_data.append(frame)\n",
        "        ret, frame = cap.read()\n",
        "    #print(\"original # frames: %d\"%(len(video_data)))\n",
        "    assert len(video_data) >= topN\n",
        "    video_data = video_data[:topN]\n",
        "    return video_data"
      ],
      "metadata": {
        "id": "TQuRMakG5GJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfs = transforms.Compose([\n",
        "            transforms.Resize((720,1280)),\n",
        "            transforms.ToTensor(),\n",
        "            # normalize,\n",
        "        ])"
      ],
      "metadata": {
        "id": "u9qZtzGRtyl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create the dataset "
      ],
      "metadata": {
        "id": "bYJBDmwvfj8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "videos_directory = \"/content/drive/MyDrive/Vision and Perception Project/Car Crash Dataset/videos/Crash-1500/\"\n",
        "annotations_file = \"/content/drive/MyDrive/Vision and Perception Project/Car Crash Dataset/videos/Crash-1500.txt\"\n",
        "#annotations_data = read_annotation_file(annotations_file)\n",
        "\n",
        "\n",
        "# for anno in annotations_data:\n",
        "#     video_file = os.path.join(videos_directory, anno['vid'] + \".mp4\")\n",
        "#     assert os.path.exists(video_file), \"video file does not exist!\" + video_file\n",
        "#     # read frames\n",
        "#     frames = get_video_frames(video_file, topN=50)\n",
        "#     labels = anno['label']\n",
        "#     #dataset = [dataset,frames]\n",
        "#     # print(\"file: %s, # frames: %d, # labels: %d.\"%(video_file, len(frames), len(labels)))\n",
        "#     # print(len(labels))\n",
        "#     # for idx, im in enumerate(frames):\n",
        "#     #     if labels[idx] == 1:\n",
        "#     #         cv2.putText(im, 'Accident', (int(im.shape[1] / 2)-60, 60), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "#     #     else:\n",
        "#     #         cv2.putText(im, 'Normal', (int(im.shape[1] / 2)-60, 60), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 255), 2, cv2.LINE_AA)\n",
        "#     #     cv2_imshow( im)\n",
        "    #     cv2.waitKey(100)"
      ],
      "metadata": {
        "id": "W5vz6-CcTXcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CCDVideos(Dataset):\n",
        "    def __init__(self, videos_directory, annotations_path):\n",
        "        super().__init__()\n",
        "        self.videos_paths = [ videos_directory + video_name for video_name in os.listdir(videos_directory) ]\n",
        "        self.videos_paths.sort()\n",
        "        self.annotations = read_annotations_file(annotations_path)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_frames = get_video_frames(self.videos_paths[idx])\n",
        "        video_annotations = self.annotations[idx]\n",
        "        return {'frames': video_frames, 'annotations': video_annotations}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.videos_paths)\n",
        "\n",
        "    def read_annotations_file(annotations_path):\n",
        "        assert os.path.exists(annotations_path), \"Annotation file does not exist!\" + annotations_path\n",
        "        result = []\n",
        "        with open(annotations_path, 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                items = {}\n",
        "                items['vid'] = line.strip().split(',[')[0]\n",
        "                labels = line.strip().split(',[')[1].split('],')[0]\n",
        "                items['label'] = [int(val) for val in labels.split(',')]\n",
        "                others = line.strip().split(',[')[1].split('],')[1].split(',')\n",
        "                items['startframe'], items['vid_ytb'], items['lighting'], items['weather'], items['ego_involve'] = others\n",
        "                result.append(items)\n",
        "        f.close()\n",
        "        return result\n",
        "\n",
        "# X: A tensor of size (B =batch_size, T =num_channels, C =seq_len, H = height, W = width)\n",
        "\n",
        "    def get_video_frames(video_file, topN=50):\n",
        "        # get the video data\n",
        "        cap = cv2.VideoCapture(video_file)\n",
        "        ret, frame = cap.read()\n",
        "        \n",
        "        video_data = []\n",
        "        while (ret):\n",
        "            video_data.append(frame)\n",
        "            ret, frame = cap.read()\n",
        "        #print(\"original # frames: %d\"%(len(video_data)))\n",
        "        assert len(video_data) >= topN\n",
        "        video_data = video_data[:topN]\n",
        "        return video_data\n",
        "        # num_channels = video_data[0].shape[2]\n",
        "        # seq_len = len(video_data)\n",
        "        # height = video_data[0].shape[0]\n",
        "        # width = video_data[0].shape[1]\n",
        "        # video_data_np_array = np.zeros(num_channels, seq_len, height, width)\n",
        "        # for i,video in  enumerate(video_data):\n",
        "        #     video_data_np_array[:,i,:,:] = video\n",
        "        # return video_data_np_array"
      ],
      "metadata": {
        "id": "btk1Dcq1fQIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir = '/content/drive/MyDrive/Vision and Perception Project/Car Crash Dataset/videos/Test-Crash/'\n",
        "anno = '/content/drive/MyDrive/Vision and Perception Project/Car Crash Dataset/videos/test_annontations.txt'\n",
        "dset = CCDVideos(dir ,anno)\n",
        "loader = DataLoader(dset, batch_size=1, num_workers=0)"
      ],
      "metadata": {
        "id": "-_WkXksri8MO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,batch in enumerate(loader):\n",
        "    print(batch['frames'][0].size())"
      ],
      "metadata": {
        "id": "GopLfalptLSw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1e5abfb-6369-4b3b-f4b6-002d6a147f1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 720, 1280, 3])\n",
            "torch.Size([1, 720, 1280, 3])\n",
            "torch.Size([1, 720, 1280, 3])\n",
            "torch.Size([1, 720, 1280, 3])\n",
            "torch.Size([1, 720, 1280, 3])\n",
            "torch.Size([1, 720, 1280, 3])\n",
            "torch.Size([1, 720, 1280, 3])\n",
            "torch.Size([1, 720, 1280, 3])\n",
            "torch.Size([1, 720, 1280, 3])\n",
            "torch.Size([1, 720, 1280, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Original ConvLSTM cell as proposed by Shi et al.\n",
        "class ConvLSTMCell(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, \n",
        "    kernel_size, padding, activation, frame_size):\n",
        "        \"\"\"\n",
        "        Initialize ConvLSTM cell.\n",
        "        Parameters\n",
        "        ----------\n",
        "        in_channels: int\n",
        "            Number of channels of input tensor.\n",
        "        out_channels: int\n",
        "            Number of channels of hidden state.\n",
        "        kernel_size: (int, int)\n",
        "            Size of the convolutional kernel.\n",
        "        \"\"\"\n",
        "\n",
        "        super(ConvLSTMCell, self).__init__()  \n",
        "\n",
        "        if activation == \"tanh\":\n",
        "            self.activation = torch.tanh \n",
        "        elif activation == \"relu\":\n",
        "            self.activation = torch.relu\n",
        "        \n",
        "        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
        "        self.conv = nn.Conv2d(in_channels=in_channels + out_channels, \n",
        "                              out_channels=4 * out_channels, \n",
        "                              kernel_size=kernel_size, \n",
        "                              padding=padding)           \n",
        "\n",
        "        # Initialize weights for Hadamard Products\n",
        "        self.W_ci = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n",
        "        self.W_co = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n",
        "        self.W_cf = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n",
        "\n",
        "    def forward(self, X, H_prev, C_prev):\n",
        "        \"\"\"\n",
        "        The following are all 3D tensors: first dimension is n° of channels/filters\n",
        "        while second and third are Height and Width of a frame.\n",
        "        i: input gate\n",
        "        f: forget gate\n",
        "        o: output gate\n",
        "        C: cell state\n",
        "        H: hidden state\n",
        "        X: inputs\n",
        "\n",
        "\n",
        "        W: weights matrices. They are a learnable set of kernels.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
        "        conv_output = self.conv(torch.cat([X, H_prev], dim=1))\n",
        "        print('conv_output.size = ',conv_output.size())\n",
        "\n",
        "        # Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
        "        i_conv, f_conv, C_conv, o_conv = torch.chunk(conv_output, chunks=4, dim=1)\n",
        "        \n",
        "        print('i_conv.size() =', i_conv.size())\n",
        "\n",
        "\n",
        "        input_gate = torch.sigmoid(i_conv + self.W_ci * C_prev )\n",
        "        forget_gate = torch.sigmoid(f_conv + self.W_cf * C_prev )\n",
        "\n",
        "        # Current Cell output\n",
        "        C = forget_gate*C_prev + input_gate * self.activation(C_conv)\n",
        "\n",
        "        output_gate = torch.sigmoid(o_conv + self.W_co * C )\n",
        "\n",
        "        # Current Hidden State\n",
        "        H = output_gate * self.activation(C)\n",
        "\n",
        "        return H, C\n",
        "\n",
        "\n",
        "class ConvLSTM(nn.Module): \n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "        input_dim: Number of channels in input\n",
        "        hidden_dim: Number of hidden channels\n",
        "        kernel_size: Size of kernel in convolutions\n",
        "        num_layers: Number of LSTM layers stacked on each other\n",
        "        batch_first: Whether or not dimension 0 is the batch or not\n",
        "        bias: Bias or no bias in Convolution\n",
        "        return_all_layers: Return the list of computations for all layers\n",
        "        Note: Will do same padding.\n",
        "    Input:\n",
        "       X: A tensor of size (B =batch_size, T =num_channels, C =seq_len, H = height, W = width)\n",
        "    Output:\n",
        "        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n",
        "            0 - layer_output_list is the list of lists of length T of each output\n",
        "            1 - last_state_list is the list of last states\n",
        "                    each element of the list is a tuple (h, c) for hidden state and memory\n",
        "    Example:\n",
        "        >> x = torch.rand((32, 10, 64, 128, 128))\n",
        "        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n",
        "        >> _, last_states = convlstm(x)\n",
        "        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, \n",
        "    kernel_size, padding, activation, frame_size):\n",
        "\n",
        "        super(ConvLSTM, self).__init__()\n",
        "\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        # We will unroll this over time steps. Remember that convLSTMcell returns\n",
        "        # H hidden state and C cell\n",
        "        self.convLSTMcell = ConvLSTMCell(in_channels, out_channels, \n",
        "        kernel_size, padding, activation, frame_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        # X is a frame sequence (B =batch_size, T =num_channels, C =seq_len, H = height, W = width)\n",
        "        # Get the dimensions\n",
        "        batch_size, _, seq_len, height, width = X.size()\n",
        "        # Initialize output\n",
        "\n",
        "        output = torch.zeros(batch_size, self.out_channels, seq_len, \n",
        "        height, width, device=device)\n",
        "        \n",
        "        # Initialize Hidden State\n",
        "        H = torch.zeros(batch_size, self.out_channels, \n",
        "        height, width, device=device)\n",
        "        print('H.size= ', H.size())\n",
        "\n",
        "        # Initialize Cell Input\n",
        "        C = torch.zeros(batch_size,self.out_channels, \n",
        "        height, width, device=device)\n",
        "        print('C.size() = ', C.size())\n",
        "\n",
        "        # Unroll over time steps\n",
        "        for time_step in range(seq_len):\n",
        "\n",
        "            H, C = self.convLSTMcell(X[:,:,time_step], H, C)\n",
        "\n",
        "            output[:,:,time_step] = H\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "\n",
        "    def __init__(self, num_channels, num_kernels, kernel_size, padding, \n",
        "    activation, frame_size, num_layers):\n",
        "\n",
        "        super(Seq2Seq, self).__init__()\n",
        "\n",
        "        self.sequential = nn.Sequential()\n",
        "\n",
        "        # Add First layer (Different in_channels than the rest)\n",
        "        self.sequential.add_module(\n",
        "            \"convlstm1\", ConvLSTM(\n",
        "                in_channels=num_channels, out_channels=num_kernels,\n",
        "                kernel_size=kernel_size, padding=padding, \n",
        "                activation=activation, frame_size=frame_size)\n",
        "        )\n",
        "\n",
        "        self.sequential.add_module(\n",
        "            \"batchnorm1\", nn.BatchNorm3d(num_features=num_kernels)\n",
        "        ) \n",
        "\n",
        "        # Add rest of the layers\n",
        "        for l in range(2, num_layers+1):\n",
        "\n",
        "            self.sequential.add_module(\n",
        "                f\"convlstm{l}\", ConvLSTM(\n",
        "                    in_channels=num_kernels, out_channels=num_kernels,\n",
        "                    kernel_size=kernel_size, padding=padding, \n",
        "                    activation=activation, frame_size=frame_size)\n",
        "                )\n",
        "                \n",
        "            self.sequential.add_module(\n",
        "                f\"batchnorm{l}\", nn.BatchNorm3d(num_features=num_kernels)\n",
        "                ) \n",
        "\n",
        "        # Add Convolutional Layer to predict output frame\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels=num_kernels, out_channels=num_channels,\n",
        "            kernel_size=kernel_size, padding=padding)\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        # Forward propagation through all the layers\n",
        "        output = self.sequential(X)\n",
        "\n",
        "        # Return only the last output frame\n",
        "        output = self.conv(output[:,:,-1])\n",
        "        outpu = nn.Sigmoid()(output)\n",
        "        \n",
        "        return output"
      ],
      "metadata": {
        "id": "RMfN4x-__KXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TRAINING\n",
        "From here on I'm gonna use pytorch tutorial for train the network\n"
      ],
      "metadata": {
        "id": "JAsmRnAADQPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The input video frames are grayscale, thus single channel\n",
        "model = Seq2Seq(num_channels=3, num_kernels=64, \n",
        "kernel_size=(5, 5), padding=(2,2), activation=\"relu\", \n",
        "frame_size=(720, 1280), num_layers=3).to(device)\n",
        "\n",
        "optim = Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Binary Cross Entropy, target pixel values either 0 or 1\n",
        "criterion = nn.BCELoss(reduction='sum')\n"
      ],
      "metadata": {
        "id": "5v9jXouTDZcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "torch.cuda.empty_cache()\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    \n",
        "    train_loss = 0                                                 \n",
        "    model.train()                                                  \n",
        "    for i, batch in enumerate(loader):  \n",
        "        input = batch['frames']\n",
        "        X = torch.stack(input, dim = 0).cuda()\n",
        "        #Reshape X to make compliant with the model\n",
        "        X = X.permute(1,4,0,2,3)\n",
        "        output = model(X) \n",
        "        target = batch['annotations']['label']                                    \n",
        "        loss = criterion(output.flatten(), target.flatten())       \n",
        "        loss.backward()                                            \n",
        "        optim.step()                                               \n",
        "        optim.zero_grad()                                           \n",
        "        train_loss += loss.item()\n",
        "        \n",
        "                                       \n",
        "    train_loss /= len(loader.dataset)                       \n",
        "\n",
        "    # val_loss = 0                                                 \n",
        "    # model.eval()                                                   \n",
        "    # with torch.no_grad():                                          \n",
        "    #     for input, target in val_loader:                          \n",
        "    #         output = model(input)                                   \n",
        "    #         loss = criterion(output.flatten(), target.flatten())   \n",
        "    #         val_loss += loss.item()                                \n",
        "    # val_loss /= len(val_loader.dataset)                            \n",
        "\n",
        "    print(\"Epoch:{} Training Loss:{:.2f} Validation Loss:{:.2f}\\n\".format(\n",
        "        epoch, train_loss, val_loss))"
      ],
      "metadata": {
        "id": "o4uJcnPaLV8U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 883
        },
        "outputId": "aac618c9-d3cb-4d96-f1d8-45eb41a463d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "H.size=  torch.Size([1, 64, 720, 1280])\n",
            "C.size() =  torch.Size([1, 64, 720, 1280])\n",
            "conv_output.size =  torch.Size([1, 256, 720, 1280])\n",
            "i_conv.size() = torch.Size([1, 64, 720, 1280])\n",
            "conv_output.size =  torch.Size([1, 256, 720, 1280])\n",
            "i_conv.size() = torch.Size([1, 64, 720, 1280])\n",
            "conv_output.size =  torch.Size([1, 256, 720, 1280])\n",
            "i_conv.size() = torch.Size([1, 64, 720, 1280])\n",
            "conv_output.size =  torch.Size([1, 256, 720, 1280])\n",
            "i_conv.size() = torch.Size([1, 64, 720, 1280])\n",
            "conv_output.size =  torch.Size([1, 256, 720, 1280])\n",
            "i_conv.size() = torch.Size([1, 64, 720, 1280])\n",
            "conv_output.size =  torch.Size([1, 256, 720, 1280])\n",
            "i_conv.size() = torch.Size([1, 64, 720, 1280])\n",
            "conv_output.size =  torch.Size([1, 256, 720, 1280])\n",
            "i_conv.size() = torch.Size([1, 64, 720, 1280])\n",
            "conv_output.size =  torch.Size([1, 256, 720, 1280])\n",
            "i_conv.size() = torch.Size([1, 64, 720, 1280])\n",
            "conv_output.size =  torch.Size([1, 256, 720, 1280])\n",
            "i_conv.size() = torch.Size([1, 64, 720, 1280])\n",
            "conv_output.size =  torch.Size([1, 256, 720, 1280])\n",
            "i_conv.size() = torch.Size([1, 64, 720, 1280])\n",
            "conv_output.size =  torch.Size([1, 256, 720, 1280])\n",
            "i_conv.size() = torch.Size([1, 64, 720, 1280])\n",
            "conv_output.size =  torch.Size([1, 256, 720, 1280])\n",
            "i_conv.size() = torch.Size([1, 64, 720, 1280])\n",
            "conv_output.size =  torch.Size([1, 256, 720, 1280])\n",
            "i_conv.size() = torch.Size([1, 64, 720, 1280])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-6a7a5d82727a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#Reshape X to make compliant with the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-423b728a9ecc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# Forward propagation through all the layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;31m# Return only the last output frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-423b728a9ecc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtime_step\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvLSTMcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-423b728a9ecc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, H_prev, C_prev)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# Idea adapted from https://github.com/ndrplz/ConvLSTM_pytorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mconv_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_prev\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conv_output.size = '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconv_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 900.00 MiB (GPU 0; 39.59 GiB total capacity; 33.96 GiB already allocated; 708.19 MiB free; 37.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "G = torch.zeros((1,3,5,7,12))\n",
        "seqlenn = 5\n",
        "for timestepp in range(seqlenn):\n",
        "    mario = G[:,:,timestepp]\n",
        "    print(mario.size())\n",
        "    break"
      ],
      "metadata": {
        "id": "kwDOeyqFaCoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q6bNwmgzjr7p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}