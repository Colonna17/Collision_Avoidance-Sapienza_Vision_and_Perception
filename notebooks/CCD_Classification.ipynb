{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "dO0eyxKsgavT"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Colonna17/Collision_Avoidance-Sapienza_Vision_and_Perception/blob/main/notebooks/CCD_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.init()"
      ],
      "metadata": {
        "id": "atiotm-qXO2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "L-0MbHDLyFCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --recurse-submodules https://github.com/Colonna17/Collision_Avoidance-Sapienza_Vision_and_Perception.git\n",
        "\n",
        "%cd Collision_Avoidance-Sapienza_Vision_and_Perception\n",
        "!./setup.sh \n",
        "# %pip install -qr external/yolov5/requirements.txt \n",
        "# %pip install -qr requirements.txt #install dependencies"
      ],
      "metadata": {
        "id": "PqfPdaTd1VwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "u4MzybKJ8Xh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from IPython.utils import io\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader,random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "sys.path.append('external/yolov5')\n",
        "from external.yolov5.utils.dataloaders import LoadImages\n",
        "sys.path.append('external/Yolov5_StrongSORT_OSNet')\n",
        "from src.utils import numpy_brg_to_rgb, custom_from_numpy\n",
        "from src.tracker import Tracker\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device = torch.device('cpu') # ToDo: just for the moment. risolvere problema classificatore\n",
        "device"
      ],
      "metadata": {
        "id": "t9Lx46qyL3-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "aV15nzv2oaKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the dataset "
      ],
      "metadata": {
        "id": "bYJBDmwvfj8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/drive/MyDrive/Vision and Perception Project/Car Crash Dataset\n",
        "data_dir = '/content/drive/MyDrive/Vision and Perception Project/Car Crash Dataset/videos'\n",
        "\n",
        "# Cosa cambia?\n",
        "# test_video_dir = data_dir + \"/Crash-1500/\"\n",
        "test_video_dir = data_dir + '/Dataset/'\n",
        "# test_annotation_file = data_dir + \"/Crash-1500.txt\"\n",
        "test_annotation_file = data_dir + '/Dataset_annotations.txt'"
      ],
      "metadata": {
        "id": "W5vz6-CcTXcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/Vision\\ and\\ Perception\\ Project/Car\\ Crash\\ Dataset/videos # data_dir"
      ],
      "metadata": {
        "id": "BgZe4Nzo72I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# videos_directory = test_video_dir\n",
        "# videos_paths = [ videos_directory + video_name for video_name in os.listdir(videos_directory) ]\n",
        "# videos_paths"
      ],
      "metadata": {
        "id": "ohsE5iXAjyFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CCDVideos(Dataset):\n",
        "    def __init__(self, videos_directory, annotations_path):\n",
        "        super().__init__()\n",
        "        self.videos_paths = [ videos_directory + video_name for video_name in os.listdir(videos_directory) ]\n",
        "        self.videos_paths.sort()\n",
        "        self.annotations = self.read_annotations_file(annotations_path)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_frames = self.videos_paths[idx] # self.get_video_frames(self.videos_paths[idx])\n",
        "        video_annotations = self.annotations[idx]\n",
        "        return {'frames': video_frames, 'annotations': video_annotations}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.videos_paths)\n",
        "\n",
        "    def read_annotations_file(self, annotations_path):\n",
        "        assert os.path.exists(annotations_path), \"Annotation file does not exist!\" + annotations_path\n",
        "        result = []\n",
        "        with open(annotations_path, 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                items = {}\n",
        "                items['vid'] = line.strip().split(',[')[0]\n",
        "                labels = line.strip().split(',[')[1].split('],')[0]\n",
        "                items['labels'] = [int(val) for val in labels.split(',')]\n",
        "                others = line.strip().split(',[')[1].split('],')[1].split(',')\n",
        "                items['startframe'], items['vid_ytb'], items['lighting'], items['weather'], items['ego_involve'] = others\n",
        "                result.append(items)\n",
        "        f.close()\n",
        "        return result\n",
        "    \n",
        "    @staticmethod\n",
        "    def get_video_frames(video_file, topN=50):\n",
        "        # get the video data\n",
        "        cap = cv2.VideoCapture(video_file)\n",
        "        ret, frame = cap.read() # Cos'Ã¨ ret?\n",
        "        \n",
        "        video_data = []\n",
        "        while (ret):\n",
        "            video_data.append(numpy_brg_to_rgb(frame))\n",
        "            ret, frame = cap.read()\n",
        "        \n",
        "        # Necessario?\n",
        "        assert len(video_data) >= topN \n",
        "        video_data = video_data[:topN]\n",
        "        \n",
        "        return video_data"
      ],
      "metadata": {
        "id": "btk1Dcq1fQIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ccd_dataset = CCDVideos(test_video_dir, test_annotation_file)\n",
        "# loader = DataLoader(ccd_dataset, batch_size=1) #, num_workers=0)\n",
        "\n",
        "train_set, validation_set, test_set = random_split(ccd_dataset,[0.8, 0.1, 0.1],\n",
        "                                                   generator=torch.Generator().manual_seed(38))\n",
        "train_loader = DataLoader(train_set, batch_size=1) #, num_workers=0)\n",
        "validation_loader = DataLoader(validation_set, batch_size=1) #, num_workers=0)\n",
        "test_loader = DataLoader(test_set, batch_size=1) #, num_workers=0)"
      ],
      "metadata": {
        "id": "-_WkXksri8MO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_test = train_set[0]\n",
        "frames_test = CCDVideos.get_video_frames(video_test['frames'])\n",
        "frame0_nr = 0\n",
        "frame1_nr = 10"
      ],
      "metadata": {
        "id": "8BkS0zWDqaVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img0_test = frames_test[frame0_nr]\n",
        "plt.imshow(img0_test)\n",
        "print('Annotation: ', video_test['annotations']['labels'][frame0_nr])"
      ],
      "metadata": {
        "id": "8frZwJcPnCoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img1_test = frames_test[frame1_nr]\n",
        "plt.imshow(img1_test)\n",
        "print('Annotation: ', video_test['annotations']['labels'][frame1_nr])"
      ],
      "metadata": {
        "id": "NHHvZ5ffm4kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optical Flow test (Just for fun)"
      ],
      "metadata": {
        "id": "ky2vDCf8bRS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.utils import flow_to_image\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "from src.opticalFlowEstimator import OpticalFlowEstimator"
      ],
      "metadata": {
        "id": "k8HzIAImpxFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optical = OpticalFlowEstimator(device)\n",
        "optical.eval()\n",
        "print('')"
      ],
      "metadata": {
        "id": "9e2k25udlY0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frame0 = torch.permute(custom_from_numpy(img0_test, device), (2,0,1)).unsqueeze(0)\n",
        "frame1 = torch.permute(custom_from_numpy(img1_test, device), (2,0,1)).unsqueeze(0)\n",
        "\n",
        "test_flow = optical(frame0, frame1)[-1]\n",
        "\n",
        "test_flow = flow_to_image(test_flow).squeeze(0)\n",
        "plt.imshow(F.to_pil_image(test_flow.to(\"cpu\")))\n",
        "\n",
        "print('Frame size: ', frame0.squeeze(0).shape)\n",
        "print('Flow size: ', test_flow.shape, '\\n')"
      ],
      "metadata": {
        "id": "2hsiX62V3m3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "av5LGIid0-IC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HParams():\n",
        "    # Model parameters\n",
        "    img_size = (720, 1280, 3)\n",
        "    lstm_hidden_dim = 32\n",
        "    collision_threshold = 0.5\n",
        "    \n",
        "    # Training parameters\n",
        "    lr = 1e-4\n",
        "    num_epochs = 5\n",
        "    early_stopping = True\n",
        "    patience = 0\n",
        "\n",
        "params = HParams()"
      ],
      "metadata": {
        "id": "THsnPEZzKizB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define model settings"
      ],
      "metadata": {
        "id": "LV1-LAiJvJss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from src.classifier import Classifier\n",
        "from src.build import build\n",
        "from src.utils import DEFAULT_OPTIONS\n",
        "\n",
        "options = DEFAULT_OPTIONS"
      ],
      "metadata": {
        "id": "zU1VaBVuon0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in options:\n",
        "    print(key, ': ', str(options[key]))"
      ],
      "metadata": {
        "id": "UvfaPcjivM8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change options here:\n"
      ],
      "metadata": {
        "id": "i5G4io3XwcuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model loading"
      ],
      "metadata": {
        "id": "ndbYP6M8qYRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yolo, tracker, classifier = build(device, options)"
      ],
      "metadata": {
        "id": "9XIAJ0-oos9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifier Training\n"
      ],
      "metadata": {
        "id": "JAsmRnAADQPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optim = torch.optim.Adam(classifier.parameters(), lr=params.lr)"
      ],
      "metadata": {
        "id": "5v9jXouTDZcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, train_dataset, valid_dataset, device, options, yolo, tracker, hparams):\n",
        "    torch.cuda.empty_cache()\n",
        "    train_history = []\n",
        "    valid_history = []\n",
        "    yolo_scaled_img_size = (options['yolo_img_height'], options['yolo_img_width'])\n",
        "    patience = hparams.patience\n",
        "\n",
        "    for epoch in range(hparams.num_epochs):\n",
        "        print('Epoch {:03d}:'.format(epoch + 1))\n",
        "\n",
        "        epoch_loss = 0.0\n",
        "        model.train() \n",
        "\n",
        "        for step, video in enumerate(train_dataset):\n",
        "            print('\\r\\t[training] Video nr: ', step, end=' ')\n",
        "            frames_path = video['frames']\n",
        "            frames = LoadImages(frames_path, img_size=yolo_scaled_img_size, stride=yolo.model.stride, auto=yolo.model.pt)\n",
        "            labels = torch.FloatTensor(video['annotations']['labels'][1:]).to(device)\n",
        "            \n",
        "            # Tracker reset (suppressing the print)\n",
        "            with io.capture_output() as captured:\n",
        "                tracker = Tracker(device, options)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            predictions = []\n",
        "            curr_frame, prev_frame = None, None\n",
        "            h = None\n",
        "            for frame_idx, (path, img_scaled, img, vid_cap, s) in enumerate(frames):\n",
        "                curr_frame = img.copy()\n",
        "                curr_frame_scaled = custom_from_numpy(img_scaled, device).unsqueeze(0)\n",
        "\n",
        "                yolo_detections = yolo.detect(curr_frame_scaled)\n",
        "                tracking_output = tracker.track(curr_frame, prev_frame, yolo_detections, curr_frame_scaled.shape[2:])\n",
        "                \n",
        "                if(frame_idx > 1): \n",
        "                    pred, h = classifier(curr_frame, prev_frame, tracking_output, h)\n",
        "                elif(frame_idx == 1): \n",
        "                    pred, h = classifier(curr_frame, prev_frame, tracking_output)\n",
        "                else: \n",
        "                    prev_frame = curr_frame\n",
        "                    continue\n",
        "                    \n",
        "                predictions.append(pred) \n",
        "                prev_frame = curr_frame\n",
        "\n",
        "            predictions = torch.cat(predictions, 0).squeeze(1)\n",
        "\n",
        "            sample_loss = model.loss(predictions, labels)\n",
        "            sample_loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += sample_loss.item()\n",
        "        \n",
        "        avg_epoch_loss = epoch_loss / len(train_dataset)\n",
        "        train_history.append(avg_epoch_loss)\n",
        "        print('\\r\\tTrain loss = {:0.4f}'.format(avg_epoch_loss))\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        avg_valid_loss = evaluate(model, valid_dataset, yolo, device, options)\n",
        "        valid_history.append(avg_valid_loss)\n",
        "        print('\\tValid loss = {:0.4f}'.format(avg_valid_loss))\n",
        "        if hparams.early_stopping:\n",
        "            stop = epoch > 0 and valid_history[-1] > valid_history[-2]\n",
        "            if stop:\n",
        "                if patience <= 0:\n",
        "                    print('\\tEarly stop.')\n",
        "                    break\n",
        "                else:\n",
        "                    print('\\tPatience.')\n",
        "                    patience -= 1\n",
        "    \n",
        "    print('Done!')\n",
        "\n",
        "    return {'train_history': train_history, 'valid_history': valid_history}\n"
      ],
      "metadata": {
        "id": "Jur0G0pduqcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataset, yolo, device, options):\n",
        "    loss = 0.0\n",
        "    model.eval()\n",
        "    yolo_scaled_img_size = (options['yolo_img_height'], options['yolo_img_width'])\n",
        "    with torch.no_grad():\n",
        "        for step, video in enumerate(dataset):\n",
        "            print('\\r\\t[evaluation] Video nr: ', step, end=' ')\n",
        "\n",
        "            # Tracker reset (suppressing the print)\n",
        "            with io.capture_output() as captured:\n",
        "                tracker = Tracker(device, options)\n",
        "\n",
        "            frames_path = video['frames']\n",
        "            frames = LoadImages(frames_path, img_size=yolo_scaled_img_size, stride=yolo.model.stride, auto=yolo.model.pt)\n",
        "            labels = torch.FloatTensor(video['annotations']['labels'][1:]).to(device)\n",
        "            \n",
        "            predictions = []\n",
        "            curr_frame, prev_frame = None, None\n",
        "            h = None\n",
        "            for frame_idx, (path, img_scaled, img, vid_cap, s) in enumerate(frames):\n",
        "                curr_frame = img.copy()\n",
        "                curr_frame_scaled = custom_from_numpy(img_scaled, device).unsqueeze(0)\n",
        "                yolo_detections = yolo.detect(curr_frame_scaled)\n",
        "                tracking_output = tracker.track(curr_frame, prev_frame, yolo_detections, curr_frame_scaled.shape[2:])\n",
        "                if(frame_idx > 1): \n",
        "                    pred, h = classifier(curr_frame, prev_frame, tracking_output, h)\n",
        "                elif(frame_idx == 1): \n",
        "                    pred, h = classifier(curr_frame, prev_frame, tracking_output)\n",
        "                else: \n",
        "                    prev_frame = curr_frame\n",
        "                    continue\n",
        "                predictions.append(pred) \n",
        "                prev_frame = curr_frame\n",
        "            predictions = torch.cat(predictions, 0).squeeze(1)\n",
        "\n",
        "            sample_loss = model.loss(predictions, labels)\n",
        "            loss += sample_loss.item()\n",
        "    torch.cuda.empty_cache()\n",
        "    return loss / len(dataset)\n"
      ],
      "metadata": {
        "id": "ADvwYCKW9dKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "logs = train(classifier, optim, train_set, validation_set, device, options, yolo, tracker, params)"
      ],
      "metadata": {
        "id": "xCXjlDYs0Xik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to free some memory\n",
        "classifier.eval()\n",
        "optim.zero_grad()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "igZzYCj7PMgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights_name = \"Crash_Classfier_weights_5.pt\"\n",
        "torch.save(classifier.state_dict(), \"/content/drive/MyDrive/Vision and Perception Project/Classifier_weights/\" + weights_name)\n",
        "\n",
        " # just to be sure:\n",
        "torch.save(classifier.state_dict(), \"/content/drive/MyDrive/Vision and Perception Project/Classifier_weights_all_/\" + weights_name)"
      ],
      "metadata": {
        "id": "I_YL_iSnp_JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_logs(logs, title, out_img_name):\n",
        "    plt.figure(figsize=(15,10))\n",
        "\n",
        "    plt.plot(list(range(len(logs['train_history']))), logs['train_history'], label='Train loss')\n",
        "    # plt.plot(list(range(len(logs['valid_history']))), logs['valid_history'], label='Valid loss')\n",
        "    \n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.savefig(out_img_name)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "UIWsMwm6DoY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logs"
      ],
      "metadata": {
        "id": "cakM0Kz_EP0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_logs(logs, 'Train vs Valid loss', 'loss_plot.png')"
      ],
      "metadata": {
        "id": "mvPNB0y_Dtfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "yGDJB2-kmsUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_loss = evaluate(classifier, test_set, yolo, device, options)\n",
        "print('Final loss: ', final_loss)"
      ],
      "metadata": {
        "id": "pC0iH_ulmto4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}