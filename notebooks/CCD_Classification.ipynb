{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "dO0eyxKsgavT"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Colonna17/Collision_Avoidance-Sapienza_Vision_and_Perception/blob/main/notebooks/CCD_Classification_flow%26lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "L-0MbHDLyFCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --recurse-submodules https://github.com/Colonna17/Collision_Avoidance-Sapienza_Vision_and_Perception.git\n",
        "\n",
        "%cd Collision_Avoidance-Sapienza_Vision_and_Perception\n",
        "!./setup.sh \n",
        "# %pip install -qr external/yolov5/requirements.txt \n",
        "# %pip install -qr requirements.txt #install dependencies"
      ],
      "metadata": {
        "id": "PqfPdaTd1VwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "u4MzybKJ8Xh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from IPython.utils import io\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader,random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "sys.path.append('external/yolov5')\n",
        "from external.yolov5.utils.dataloaders import LoadImages\n",
        "sys.path.append('external/Yolov5_StrongSORT_OSNet')\n",
        "from src.utils import numpy_brg_to_rgb, custom_from_numpy\n",
        "from src.tracker import Tracker\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device = torch.device('cpu') # ToDo: just for the moment. risolvere problema classificatore\n",
        "device"
      ],
      "metadata": {
        "id": "t9Lx46qyL3-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "aV15nzv2oaKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the dataset "
      ],
      "metadata": {
        "id": "bYJBDmwvfj8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/drive/MyDrive/Vision and Perception Project/Car Crash Dataset\n",
        "data_dir = '/content/drive/MyDrive/Vision and Perception Project/Car Crash Dataset/videos'\n",
        "\n",
        "# Cosa cambia?\n",
        "# test_video_dir = data_dir + \"/Crash-1500/\"\n",
        "test_video_dir = data_dir + '/Dataset/'\n",
        "# test_annotation_file = data_dir + \"/Crash-1500.txt\"\n",
        "test_annotation_file = data_dir + '/Dataset_annotations.txt'"
      ],
      "metadata": {
        "id": "W5vz6-CcTXcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/Vision\\ and\\ Perception\\ Project/Car\\ Crash\\ Dataset/videos # data_dir"
      ],
      "metadata": {
        "id": "BgZe4Nzo72I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# videos_directory = test_video_dir\n",
        "# videos_paths = [ videos_directory + video_name for video_name in os.listdir(videos_directory) ]\n",
        "# videos_paths"
      ],
      "metadata": {
        "id": "ohsE5iXAjyFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CCDVideos(Dataset):\n",
        "    def __init__(self, videos_directory, annotations_path):\n",
        "        super().__init__()\n",
        "        self.videos_paths = [ videos_directory + video_name for video_name in os.listdir(videos_directory) ]\n",
        "        self.videos_paths.sort()\n",
        "        self.annotations = self.read_annotations_file(annotations_path)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_frames = self.videos_paths[idx] # self.get_video_frames(self.videos_paths[idx])\n",
        "        video_annotations = self.annotations[idx]\n",
        "        return {'frames': video_frames, 'annotations': video_annotations}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.videos_paths)\n",
        "\n",
        "    def read_annotations_file(self, annotations_path):\n",
        "        assert os.path.exists(annotations_path), \"Annotation file does not exist!\" + annotations_path\n",
        "        result = []\n",
        "        with open(annotations_path, 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                items = {}\n",
        "                items['vid'] = line.strip().split(',[')[0]\n",
        "                labels = line.strip().split(',[')[1].split('],')[0]\n",
        "                items['labels'] = [int(val) for val in labels.split(',')]\n",
        "                others = line.strip().split(',[')[1].split('],')[1].split(',')\n",
        "                items['startframe'], items['vid_ytb'], items['lighting'], items['weather'], items['ego_involve'] = others\n",
        "                result.append(items)\n",
        "        f.close()\n",
        "        return result\n",
        "    \n",
        "    @staticmethod\n",
        "    def get_video_frames(video_file, topN=50):\n",
        "        # get the video data\n",
        "        cap = cv2.VideoCapture(video_file)\n",
        "        ret, frame = cap.read() # Cos'è ret?\n",
        "        \n",
        "        video_data = []\n",
        "        while (ret):\n",
        "            video_data.append(numpy_brg_to_rgb(frame))\n",
        "            ret, frame = cap.read()\n",
        "        \n",
        "        # Necessario?\n",
        "        assert len(video_data) >= topN \n",
        "        video_data = video_data[:topN]\n",
        "        \n",
        "        return video_data"
      ],
      "metadata": {
        "id": "btk1Dcq1fQIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ccd_dataset = CCDVideos(test_video_dir, test_annotation_file)\n",
        "# loader = DataLoader(ccd_dataset, batch_size=1) #, num_workers=0)\n",
        "\n",
        "train_set, validation_set, test_set = random_split(ccd_dataset,[0.8, 0.1, 0.1],\n",
        "                                                   generator=torch.Generator().manual_seed(38))\n",
        "train_loader = DataLoader(train_set, batch_size=1) #, num_workers=0)\n",
        "validation_loader = DataLoader(validation_set, batch_size=1) #, num_workers=0)\n",
        "test_loader = DataLoader(test_set, batch_size=1) #, num_workers=0)"
      ],
      "metadata": {
        "id": "-_WkXksri8MO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_test = train_set[0]\n",
        "frames_test = CCDVideos.get_video_frames(video_test['frames'])\n",
        "frame0_nr = 0\n",
        "frame1_nr = 10"
      ],
      "metadata": {
        "id": "8BkS0zWDqaVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img0_test = frames_test[frame0_nr]\n",
        "plt.imshow(img0_test)\n",
        "print('Annotation: ', video_test['annotations']['labels'][frame0_nr])"
      ],
      "metadata": {
        "id": "8frZwJcPnCoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img1_test = frames_test[frame1_nr]\n",
        "plt.imshow(img1_test)\n",
        "print('Annotation: ', video_test['annotations']['labels'][frame1_nr])"
      ],
      "metadata": {
        "id": "NHHvZ5ffm4kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optical Flow test (Just for fun)"
      ],
      "metadata": {
        "id": "ky2vDCf8bRS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.utils import flow_to_image\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "from src.opticalFlowEstimator import OpticalFlowEstimator"
      ],
      "metadata": {
        "id": "k8HzIAImpxFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optical = OpticalFlowEstimator(device)\n",
        "optical.eval()\n",
        "print('')"
      ],
      "metadata": {
        "id": "9e2k25udlY0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frame0 = torch.permute(custom_from_numpy(img0_test, device), (2,0,1)).unsqueeze(0)\n",
        "frame1 = torch.permute(custom_from_numpy(img1_test, device), (2,0,1)).unsqueeze(0)\n",
        "\n",
        "test_flow = optical(frame0, frame1)[-1]\n",
        "\n",
        "test_flow = flow_to_image(test_flow).squeeze(0)\n",
        "plt.imshow(F.to_pil_image(test_flow.to(\"cpu\")))\n",
        "\n",
        "print('Frame size: ', frame0.squeeze(0).shape)\n",
        "print('Flow size: ', test_flow.shape, '\\n')"
      ],
      "metadata": {
        "id": "2hsiX62V3m3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "av5LGIid0-IC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HParams():\n",
        "    # Model parameters\n",
        "    img_size = (720, 1280, 3)\n",
        "    lstm_hidden_dim = 32\n",
        "    collision_threshold = 0.5\n",
        "    \n",
        "    # Training parameters\n",
        "    lr = 1e-4\n",
        "    num_epochs = 5\n",
        "    early_stopping = True\n",
        "    patience = 0\n",
        "\n",
        "params = HParams()"
      ],
      "metadata": {
        "id": "THsnPEZzKizB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define model settings"
      ],
      "metadata": {
        "id": "LV1-LAiJvJss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from src.classifier import Classifier\n",
        "from src.build import build\n",
        "from src.utils import DEFAULT_OPTIONS\n",
        "\n",
        "options = DEFAULT_OPTIONS"
      ],
      "metadata": {
        "id": "zU1VaBVuon0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in options:\n",
        "    print(key, ': ', str(options[key]))"
      ],
      "metadata": {
        "id": "UvfaPcjivM8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change options here:\n"
      ],
      "metadata": {
        "id": "i5G4io3XwcuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model loading"
      ],
      "metadata": {
        "id": "ndbYP6M8qYRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yolo, tracker, classifier = build(device, options)"
      ],
      "metadata": {
        "id": "9XIAJ0-oos9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifier Training\n"
      ],
      "metadata": {
        "id": "JAsmRnAADQPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optim = torch.optim.Adam(classifier.parameters(), lr=params.lr)"
      ],
      "metadata": {
        "id": "5v9jXouTDZcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, train_dataset, valid_dataset, device, options, yolo, tracker, hparams):\n",
        "    torch.cuda.empty_cache()\n",
        "    train_history = []\n",
        "    valid_history = []\n",
        "    yolo_scaled_img_size = (options['yolo_img_height'], options['yolo_img_width'])\n",
        "    patience = hparams.patience\n",
        "\n",
        "    for epoch in range(hparams.num_epochs):\n",
        "        print('Epoch {:03d}:'.format(epoch + 1))\n",
        "\n",
        "        epoch_loss = 0.0\n",
        "        model.train() \n",
        "\n",
        "        for step, video in enumerate(train_dataset):\n",
        "            print('\\r\\t[training] Video nr: ', step, end=' ')\n",
        "            frames_path = video['frames']\n",
        "            frames = LoadImages(frames_path, img_size=yolo_scaled_img_size, stride=yolo.model.stride, auto=yolo.model.pt)\n",
        "            labels = torch.FloatTensor(video['annotations']['labels'][1:]).to(device)\n",
        "            \n",
        "            # Tracker reset (suppressing the print)\n",
        "            with io.capture_output() as captured:\n",
        "                tracker = Tracker(device, options)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            predictions = []\n",
        "            curr_frame, prev_frame = None, None\n",
        "            h = None\n",
        "            for frame_idx, (path, img_scaled, img, vid_cap, s) in enumerate(frames):\n",
        "                curr_frame = img.copy()\n",
        "                curr_frame_scaled = custom_from_numpy(img_scaled, device).unsqueeze(0)\n",
        "\n",
        "                yolo_detections = yolo.detect(curr_frame_scaled)\n",
        "                tracking_output = tracker.track(curr_frame, prev_frame, yolo_detections, curr_frame_scaled.shape[2:])\n",
        "                \n",
        "                if(frame_idx > 1): \n",
        "                    pred, h = classifier(curr_frame, prev_frame, tracking_output, h)\n",
        "                elif(frame_idx == 1): \n",
        "                    pred, h = classifier(curr_frame, prev_frame, tracking_output)\n",
        "                else: \n",
        "                    prev_frame = curr_frame\n",
        "                    continue\n",
        "                    \n",
        "                predictions.append(pred) \n",
        "                prev_frame = curr_frame\n",
        "\n",
        "            predictions = torch.cat(predictions, 0).squeeze(1)\n",
        "\n",
        "            sample_loss = model.loss(predictions, labels)\n",
        "            sample_loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += sample_loss.item()\n",
        "        \n",
        "        avg_epoch_loss = epoch_loss / len(train_dataset)\n",
        "        train_history.append(avg_epoch_loss)\n",
        "        print('\\r\\tTrain loss = {:0.4f}'.format(avg_epoch_loss))\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        avg_valid_loss = evaluate(model, valid_dataset, yolo, device, options)\n",
        "        valid_history.append(avg_valid_loss)\n",
        "        print('\\tValid loss = {:0.4f}'.format(avg_valid_loss))\n",
        "        if hparams.early_stopping:\n",
        "            stop = epoch > 0 and valid_history[-1] > valid_history[-2]\n",
        "            if stop:\n",
        "                if patience <= 0:\n",
        "                    print('\\tEarly stop.')\n",
        "                    break\n",
        "                else:\n",
        "                    print('\\tPatience.')\n",
        "                    patience -= 1\n",
        "    \n",
        "    print('Done!')\n",
        "\n",
        "    return {'train_history': train_history, 'valid_history': valid_history}\n"
      ],
      "metadata": {
        "id": "Jur0G0pduqcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataset, yolo, device, options):\n",
        "    loss = 0.0\n",
        "    model.eval()\n",
        "    yolo_scaled_img_size = (options['yolo_img_height'], options['yolo_img_width'])\n",
        "    with torch.no_grad():\n",
        "        for step, video in enumerate(dataset):\n",
        "            print('\\r\\t[evaluation] Video nr: ', step, end=' ')\n",
        "\n",
        "            # Tracker reset (suppressing the print)\n",
        "            with io.capture_output() as captured:\n",
        "                tracker = Tracker(device, options)\n",
        "\n",
        "            frames_path = video['frames']\n",
        "            frames = LoadImages(frames_path, img_size=yolo_scaled_img_size, stride=yolo.model.stride, auto=yolo.model.pt)\n",
        "            labels = torch.FloatTensor(video['annotations']['labels'][1:]).to(device)\n",
        "            \n",
        "            predictions = []\n",
        "            curr_frame, prev_frame = None, None\n",
        "            h = None\n",
        "            for frame_idx, (path, img_scaled, img, vid_cap, s) in enumerate(frames):\n",
        "                curr_frame = img.copy()\n",
        "                curr_frame_scaled = custom_from_numpy(img_scaled, device).unsqueeze(0)\n",
        "                yolo_detections = yolo.detect(curr_frame_scaled)\n",
        "                tracking_output = tracker.track(curr_frame, prev_frame, yolo_detections, curr_frame_scaled.shape[2:])\n",
        "                if(frame_idx > 1): \n",
        "                    pred, h = classifier(curr_frame, prev_frame, tracking_output, h)\n",
        "                elif(frame_idx == 1): \n",
        "                    pred, h = classifier(curr_frame, prev_frame, tracking_output)\n",
        "                else: \n",
        "                    prev_frame = curr_frame\n",
        "                    continue\n",
        "                predictions.append(pred) \n",
        "                prev_frame = curr_frame\n",
        "            predictions = torch.cat(predictions, 0).squeeze(1)\n",
        "\n",
        "            sample_loss = model.loss(predictions, labels)\n",
        "            loss += sample_loss.item()\n",
        "    torch.cuda.empty_cache()\n",
        "    return loss / len(dataset)\n"
      ],
      "metadata": {
        "id": "ADvwYCKW9dKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "logs = train(classifier, optim, train_set, validation_set, device, options, yolo, tracker, params)"
      ],
      "metadata": {
        "id": "xCXjlDYs0Xik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to free some memory\n",
        "classifier.eval()\n",
        "optim.zero_grad()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "igZzYCj7PMgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights_name = \"Crash_Classfier_weights_5.pt\"\n",
        "torch.save(classifier.state_dict(), \"/content/drive/MyDrive/Vision and Perception Project/Classifier_weights/\" + weights_name)\n",
        "\n",
        " # just to be sure:\n",
        "torch.save(classifier.state_dict(), \"/content/drive/MyDrive/Vision and Perception Project/Classifier_weights_all_/\" + weights_name)"
      ],
      "metadata": {
        "id": "I_YL_iSnp_JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_logs(logs, title, out_img_name):\n",
        "    plt.figure(figsize=(15,10))\n",
        "\n",
        "    plt.plot(list(range(len(logs['train_history']))), logs['train_history'], label='Train loss')\n",
        "    # plt.plot(list(range(len(logs['valid_history']))), logs['valid_history'], label='Valid loss')\n",
        "    \n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.savefig(out_img_name)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "UIWsMwm6DoY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logs"
      ],
      "metadata": {
        "id": "cakM0Kz_EP0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_logs(logs, 'Train vs Valid loss', 'loss_plot.png')"
      ],
      "metadata": {
        "id": "mvPNB0y_Dtfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "yGDJB2-kmsUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_loss = evaluate(classifier, test_set, yolo, device, options)\n",
        "print('Final loss: ', final_loss)"
      ],
      "metadata": {
        "id": "pC0iH_ulmto4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Original Model [Da togliere?]"
      ],
      "metadata": {
        "id": "dO0eyxKsgavT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Original ConvLSTM cell as proposed by Shi et al.\n",
        "# Adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
        "\n",
        "class ConvLSTMCell(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels,  kernel_size, padding, activation, frame_size):\n",
        "        \"\"\"\n",
        "        Initialize ConvLSTM cell.\n",
        "        Parameters\n",
        "        ----------\n",
        "        in_channels: int\n",
        "            Number of channels of input tensor.\n",
        "        out_channels: int\n",
        "            Number of channels of hidden state.\n",
        "        kernel_size: (int, int)\n",
        "            Size of the convolutional kernel.\n",
        "        \"\"\"\n",
        "\n",
        "        # super(ConvLSTMCell, self).__init__()  # Perchè?\n",
        "        super().__init__()  \n",
        "\n",
        "        if activation == \"tanh\":\n",
        "            self.activation = torch.tanh \n",
        "        elif activation == \"relu\":\n",
        "            self.activation = torch.relu\n",
        "        \n",
        "        self.conv = nn.Conv2d(in_channels = in_channels + out_channels, \n",
        "                              out_channels = 4 * out_channels, \n",
        "                              kernel_size = kernel_size, \n",
        "                              padding = padding)           \n",
        "\n",
        "        # Initialize weights for Hadamard Products\n",
        "        self.W_ci = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n",
        "        self.W_co = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n",
        "        self.W_cf = nn.Parameter(torch.Tensor(out_channels, *frame_size))\n",
        "\n",
        "    def forward(self, X, H_prev, C_prev):\n",
        "        \"\"\"\n",
        "        The following are all 3D tensors: first dimension is n° of channels/filters\n",
        "        while second and third are Height and Width of a frame.\n",
        "        i: input gate\n",
        "        f: forget gate\n",
        "        o: output gate\n",
        "        C: cell state\n",
        "        H: hidden state\n",
        "        X: inputs\n",
        "        W: weights matrices. They are a learnable set of kernels.\n",
        "        \"\"\"\n",
        "\n",
        "        conv_output = self.conv(torch.cat([X, H_prev], dim=1))\n",
        "\n",
        "        i_conv, f_conv, C_conv, o_conv = torch.chunk(conv_output, chunks=4, dim=1)\n",
        "\n",
        "        input_gate = torch.sigmoid(i_conv + self.W_ci * C_prev )\n",
        "        forget_gate = torch.sigmoid(f_conv + self.W_cf * C_prev )\n",
        "\n",
        "        # Current Cell output\n",
        "        C = forget_gate*C_prev + input_gate * self.activation(C_conv)\n",
        "\n",
        "        output_gate = torch.sigmoid(o_conv + self.W_co * C )\n",
        "\n",
        "        # Current Hidden State\n",
        "        H = output_gate * self.activation(C)\n",
        "\n",
        "        return H, C"
      ],
      "metadata": {
        "id": "jOFBSW1cCxG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Original ConvLSTM cell as proposed by Shi et al.\n",
        "# Adapted from https://github.com/ndrplz/ConvLSTM_pytorch\n",
        "\n",
        "class ConvLSTM(nn.Module): \n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "        input_dim: Number of channels in input\n",
        "        hidden_dim: Number of hidden channels\n",
        "        kernel_size: Size of kernel in convolutions\n",
        "        num_layers: Number of LSTM layers stacked on each other\n",
        "        batch_first: Whether or not dimension 0 is the batch or not\n",
        "        bias: Bias or no bias in Convolution\n",
        "        return_all_layers: Return the list of computations for all layers\n",
        "        Note: Will do same padding.\n",
        "    Input:\n",
        "       X: A tensor of size (B =batch_size, T =num_channels, C =seq_len, H = height, W = width)\n",
        "    Output:\n",
        "        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n",
        "            0 - layer_output_list is the list of lists of length T of each output\n",
        "            1 - last_state_list is the list of last states\n",
        "                    each element of the list is a tuple (h, c) for hidden state and memory\n",
        "    Example:\n",
        "        >> x = torch.rand((32, 10, 64, 128, 128))\n",
        "        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n",
        "        >> _, last_states = convlstm(x)\n",
        "        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding, activation, frame_size):\n",
        "        # super(ConvLSTM, self).__init__()\n",
        "        super().__init__()        \n",
        "\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        # We will unroll this over time steps. Remember that convLSTMcell returns\n",
        "        # H hidden state and C cell\n",
        "        self.convLSTMcell = ConvLSTMCell(in_channels, out_channels, kernel_size, padding, activation, frame_size)\n",
        "\n",
        "    \n",
        "    # X is a frame sequence of size (B, T, C, H, W) with (B = batch_size, T = num_channels, C = seq_len, H = height, W = width)\n",
        "    def forward(self, X):\n",
        "        # Get the dimensions\n",
        "        batch_size, _, seq_len, height, width = X.size() # what is the ignored parameter\n",
        "\n",
        "        # Initialize output\n",
        "        output = torch.zeros(batch_size, self.out_channels, seq_len, \n",
        "        height, width, device=device)\n",
        "        \n",
        "        # Initialize Hidden State\n",
        "        H = torch.zeros(batch_size, self.out_channels, \n",
        "        height, width, device=device)\n",
        "\n",
        "        # Initialize Cell Input\n",
        "        C = torch.zeros(batch_size,self.out_channels, \n",
        "        height, width, device=device)\n",
        "\n",
        "        # Unroll over time steps\n",
        "        for time_step in range(seq_len):\n",
        "\n",
        "            H, C = self.convLSTMcell(X[:,:,time_step], H, C)\n",
        "\n",
        "            output[:,:,time_step] = H\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "F5vruikjC4l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "\n",
        "    def __init__(self, num_channels, num_kernels, kernel_size, padding, activation, frame_size, num_layers):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "\n",
        "        self.sequential = nn.Sequential()\n",
        "\n",
        "        # Add First layer (Different in_channels than the rest)\n",
        "        self.sequential.add_module(\n",
        "            \"convlstm\", ConvLSTM(\n",
        "                in_channels=num_channels, out_channels=num_kernels,\n",
        "                kernel_size=kernel_size, padding=padding, \n",
        "                activation=activation, frame_size=frame_size)\n",
        "        )\n",
        "\n",
        "        # self.sequential.add_module(\n",
        "        #     \"batchnorm\", nn.BatchNorm3d(num_features=num_kernels)\n",
        "        # ) \n",
        "\n",
        "        # Add Convolutional Layer to predict output frame\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels=num_kernels, out_channels=num_channels,\n",
        "            kernel_size=kernel_size, padding=padding)\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        # Forward propagation through all the layers\n",
        "        output = self.sequential(X)\n",
        "\n",
        "        # Return only the last output frame\n",
        "        output = self.conv(output[:,:,-1])\n",
        "        outpu = nn.Sigmoid()(output)\n",
        "        \n",
        "        return output"
      ],
      "metadata": {
        "id": "RMfN4x-__KXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Roba a caso [Da togliere?]"
      ],
      "metadata": {
        "id": "q9LxlNyMDd_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install wandb\n",
        "# import wandb\n",
        "# wandb.init()"
      ],
      "metadata": {
        "id": "atiotm-qXO2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import nvidia_smi\n",
        "\n",
        "# nvidia_smi.nvmlInit()\n",
        "\n",
        "# deviceCount = nvidia_smi.nvmlDeviceGetCount()\n",
        "# for i in range(deviceCount):\n",
        "#     handle = nvidia_smi.nvmlDeviceGetHandleByIndex(i)\n",
        "#     info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
        "#     print(\"Device {}: {}, Memory : ({:.2f}% free): {}(total), {} (free), {} (used)\".format(i, nvidia_smi.nvmlDeviceGetName(handle), 100*info.free/info.total, info.total, info.free, info.used))\n",
        "\n",
        "# nvidia_smi.nvmlShutdown()"
      ],
      "metadata": {
        "id": "98w0-xusZfhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # prints currently alive Tensors and Variables\n",
        "# import torch\n",
        "# import gc\n",
        "# for obj in gc.get_objects():\n",
        "#     try:\n",
        "#         if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
        "#             print(type(obj), obj.size(), obj.element_size() * obj.nelement())\n",
        "#     except:\n",
        "#         pass\n",
        "#     del obj"
      ],
      "metadata": {
        "id": "-ShsnKl7amvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# def sizeof_fmt(num, suffix='B'):\n",
        "#     ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
        "#     for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
        "#         if abs(num) < 1024.0:\n",
        "#             return \"%3.1f %s%s\" % (num, unit, suffix)\n",
        "#         num /= 1024.0\n",
        "#     return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
        "\n",
        "# for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
        "#                          key= lambda x: -x[1])[:10]:\n",
        "#     print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
      ],
      "metadata": {
        "id": "GUWewRRuVmwY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
